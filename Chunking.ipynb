{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following imports the necessary for the chunking process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *!!! This process had to be run on Google Collab's T4 GPU as it is computationally intensive, the code was transfered here from the google collab notebook after the files were processed and saved manually into the larger folder, file must be adjusted for running outside of google collab!!!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (73.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.venv/lib/python3.12/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch nltk\n",
    "!pip uninstall torch transformers -y\n",
    "!pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following takes the cleaned files and and iterates over them whole chinking each one into semantic chunks determined using a RoBERTa model for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Function to calculate semantic similarity between sentences using RoBERTa\n",
    "def calculate_similarity(sentence_embedding, chunk_embedding):\n",
    "    return np.dot(sentence_embedding, chunk_embedding) / (np.linalg.norm(sentence_embedding) * np.linalg.norm(chunk_embedding))\n",
    "\n",
    "# Function to embed a sentence using RoBERTa\n",
    "def embed_sentence(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        sentence_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return sentence_embedding\n",
    "\n",
    "# Semantic Chunking function\n",
    "def chunk_text_semantic(sentences, tokenizer, model, max_tokens=400, similarity_threshold=0.7):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_embedding = embed_sentence(sentence, tokenizer, model)\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "            current_chunk.append(sentence)\n",
    "        else:\n",
    "            similarity = calculate_similarity(sentence_embedding, current_chunk_embedding)\n",
    "            if similarity > similarity_threshold:\n",
    "                current_chunk.append(sentence)\n",
    "                current_chunk_embedding = (current_chunk_embedding + sentence_embedding) / 2\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "        # Check if the current chunk exceeds the token limit\n",
    "        current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n",
    "        if current_chunk_tokens > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_chunk_embedding = None\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Overlap function\n",
    "def create_overlap_chunks(chunks, overlap_size=2):\n",
    "    overlapped_chunks = []\n",
    "    for i in range(len(chunks)):\n",
    "        if i > 0:\n",
    "            overlap = chunks[i-1].split()[-overlap_size:]\n",
    "            overlapped_chunks.append(\" \".join(overlap + chunks[i].split()))\n",
    "        else:\n",
    "            overlapped_chunks.append(chunks[i])\n",
    "    return overlapped_chunks\n",
    "\n",
    "# Path to the folder where the cleaned text files are stored\n",
    "folder_path = '/content/drive/MyDrive/cleaned_output_folder'  # Ensure this is the correct path\n",
    "output_folder = '/content/drive/MyDrive/semantic_chunks'  # Folder to store the chunks\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Initialize RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to read each text file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):  # Ensure only text files are processed\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        # Read the content of the text file\n",
    "        text = read_text_file(file_path)\n",
    "\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = tokenize_text(text)\n",
    "\n",
    "        # Chunk the text semantically\n",
    "        chunks = chunk_text_semantic(sentences, tokenizer, model)\n",
    "\n",
    "        # Apply overlapping to the chunks\n",
    "        overlapped_chunks = create_overlap_chunks(chunks)\n",
    "\n",
    "        # Save the semantic chunks to a file\n",
    "        chunk_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_chunks.txt\")\n",
    "        with open(chunk_file_path, 'w', encoding='utf-8') as chunk_file:\n",
    "            for chunk in overlapped_chunks:\n",
    "                chunk_file.write(chunk + \"\\n\\n\")  # Save each chunk with a double newline for separation\n",
    "\n",
    "        print(f\"Chunks of {filename} saved to {output_folder}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
